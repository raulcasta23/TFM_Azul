{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APAsEWVS6f1L"
      },
      "source": [
        "# Importar Juego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6813SMa96jLv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/TFE/Azul/')\n",
        "\n",
        "from Azul import AzulGame, AzulState\n",
        "game = AzulGame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOmALW2C6SYJ"
      },
      "source": [
        "# Codificación Estados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi-EM-8b6Wz8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def state_to_vector(state):\n",
        "    # Codificar las fábricas (agrupadas por color)\n",
        "    tile_colors = ['B', 'Y', 'R', 'K', 'W']\n",
        "    factories_vector = []\n",
        "\n",
        "    for factory in state.factories:\n",
        "        factory_encoding = [0] * len(tile_colors)\n",
        "        for tile in factory:\n",
        "            factory_encoding[tile_colors.index(tile)] += 1\n",
        "        factories_vector.extend(factory_encoding)\n",
        "\n",
        "    # Codificar el centro (agrupado por color)\n",
        "    center_vector = [0] * len(tile_colors)\n",
        "    for tile in state.center:\n",
        "        if tile in tile_colors:\n",
        "            center_vector[tile_colors.index(tile)] += 1\n",
        "    first_player_token = 1 if '1' in state.center else 0\n",
        "    center_vector.append(first_player_token)\n",
        "\n",
        "    # Codificar el tablero de los jugadores y las líneas de preparación\n",
        "    player_boards_vector = []\n",
        "    for player in state.players:\n",
        "        # Codificar el tablero\n",
        "        board_encoding = []\n",
        "        for row in player['board']:\n",
        "            row_encoding = [0 if tile == '' else tile_colors.index(tile) + 1 for tile in row]\n",
        "            board_encoding.extend(row_encoding)\n",
        "        player_boards_vector.extend(board_encoding)\n",
        "\n",
        "        # Codificar las líneas de preparación\n",
        "        pattern_lines_encoding = []\n",
        "        for pattern_line in player['pattern_lines']:\n",
        "            line_encoding = [0 if tile == '' else tile_colors.index(tile) + 1 for tile in pattern_line]\n",
        "            pattern_lines_encoding.extend(line_encoding)\n",
        "        player_boards_vector.extend(pattern_lines_encoding)\n",
        "\n",
        "        # Codificar el suelo\n",
        "        floor_encoding = [len(player['floor'])]  # Solo importa cuántas fichas hay, no el color\n",
        "        player_boards_vector.extend(floor_encoding)\n",
        "\n",
        "    # Codificar el jugador actual (one-hot encoding)\n",
        "    current_player_vector = [1 if state.current_player == i else 0 for i in range(len(state.players))]\n",
        "\n",
        "    # Concatenar todo en un vector de estado final\n",
        "    state_vector = np.array(factories_vector + center_vector + player_boards_vector + current_player_vector, dtype=np.float32)\n",
        "\n",
        "    return state_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_75hnE16Hkl"
      },
      "source": [
        "# Codificación Acciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQCEponP4Gaz"
      },
      "outputs": [],
      "source": [
        "# Parámetros del juego\n",
        "num_factories = 6  # Número de fábricas (incluyendo el centro)\n",
        "num_colors = 5     # Número de colores diferentes de fichas\n",
        "num_rows = 6       # Número de filas de preparación + fila del suelo\n",
        "\n",
        "# Diccionario para mapear colores a números\n",
        "color_mapping = {'W': 0, 'Y': 1, 'B': 2, 'R': 3, 'K': 4}\n",
        "\n",
        "def encode_action(factory_num, tile_color, row_num):\n",
        "    \"\"\"\n",
        "    Codifica la acción en un único índice usando factory_num, tile_color, y row_num.\n",
        "    \"\"\"\n",
        "    # Mapear el centro (-1) a 5 y el suelo (-1) a 5\n",
        "    if factory_num == -1:\n",
        "        factory_num = 5\n",
        "    if row_num == -1:\n",
        "        row_num = 5\n",
        "\n",
        "    # Mapear el color usando el diccionario\n",
        "    tile_color = color_mapping[tile_color]\n",
        "\n",
        "    # Codificar la acción en un índice único\n",
        "    action_index = factory_num * (num_colors * num_rows) + tile_color * num_rows + row_num\n",
        "    return action_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUqk0COi6OtT"
      },
      "outputs": [],
      "source": [
        "# Inverso del diccionario de colores\n",
        "inverse_color_mapping = {v: k for k, v in color_mapping.items()}\n",
        "\n",
        "def decode_action(action_index):\n",
        "    \"\"\"\n",
        "    Decodifica el índice único en factory_num, tile_color, y row_num.\n",
        "    \"\"\"\n",
        "    # Decodificar el índice en factory_num, tile_color y row_num\n",
        "    factory_num = action_index // (num_colors * num_rows)\n",
        "    remainder = action_index % (num_colors * num_rows)\n",
        "    tile_color = remainder // num_rows\n",
        "    row_num = remainder % num_rows\n",
        "\n",
        "    # Restaurar los valores originales\n",
        "    if factory_num == 5:\n",
        "        factory_num = -1\n",
        "    if row_num == 5:\n",
        "        row_num = -1\n",
        "\n",
        "    # Restaurar el color a la letra original\n",
        "    tile_color = inverse_color_mapping[tile_color]\n",
        "\n",
        "    return factory_num, tile_color, row_num\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws5sZHka60ns"
      },
      "source": [
        "# Configuración Agente DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEIIaQOt6OuX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A9KPSYL6y9x"
      },
      "outputs": [],
      "source": [
        "input_dim = 115  # Dimensión del vector de estado\n",
        "num_actions = 180  # Número de acciones posibles total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_oAvNmVsqBb"
      },
      "outputs": [],
      "source": [
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_dim, num_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.V = nn.Linear(128, 1)\n",
        "        self.A = nn.Linear(128, num_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        V = self.V(x)\n",
        "        A = self.A(x)\n",
        "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
        "        return Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrsf9Pssuai"
      },
      "outputs": [],
      "source": [
        "# Red principal y red objetivo\n",
        "main_nn = DuelingDQN(input_dim, num_actions).to(device)\n",
        "target_nn = DuelingDQN(input_dim, num_actions).to(device)\n",
        "target_nn.load_state_dict(main_nn.state_dict())\n",
        "\n",
        "# Función de pérdida y optimizador\n",
        "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "loss_fn = nn.SmoothL1Loss()  # Huber loss\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=500, gamma=0.97)\n",
        "min_lr = 1e-5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSPsfuMv-RNF"
      },
      "source": [
        "# Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0HSoZEN_j79"
      },
      "outputs": [],
      "source": [
        "class UniformBuffer(object):\n",
        "  \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
        "\n",
        "  def __init__(self, size, device):\n",
        "    self._size = size\n",
        "    self.buffer = []\n",
        "    self.device = device\n",
        "    self._next_idx = 0\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    if self._next_idx >= len(self.buffer):\n",
        "      self.buffer.append((state, action, reward, next_state, done))\n",
        "    else:\n",
        "      self.buffer[self._next_idx] = (state, action, reward, next_state, done)\n",
        "    self._next_idx = (self._next_idx + 1) % self._size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def sample(self, num_samples):\n",
        "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "    idx = np.random.choice(len(self.buffer), num_samples)\n",
        "    for i in idx:\n",
        "      elem = self.buffer[i]\n",
        "      state, action, reward, next_state, done = elem\n",
        "      states.append(np.array(state, copy=False))\n",
        "      actions.append(np.array(action, copy=False))\n",
        "      rewards.append(reward)\n",
        "      next_states.append(np.array(next_state, copy=False))\n",
        "      dones.append(done)\n",
        "\n",
        "    states = torch.as_tensor(np.array(states), device=self.device)\n",
        "    actions = torch.as_tensor(np.array(actions), device=self.device)\n",
        "    rewards = torch.as_tensor(np.array(rewards, dtype=np.float32),\n",
        "                              device=self.device)\n",
        "    next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
        "    dones = torch.as_tensor(np.array(dones, dtype=np.float32),\n",
        "                            device=self.device)\n",
        "\n",
        "    return states, actions, rewards, next_states, dones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utzsOFRA-Tin"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def select_action(state, epsilon, legal_actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(legal_actions) # Elige una acción aleatoria de entre las acciones legales\n",
        "    else:\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_values = main_nn(state)\n",
        "\n",
        "        # Convertir las acciones legales a índices únicos usando encode_action\n",
        "        legal_action_indices = [encode_action(*action) for action in legal_actions]\n",
        "\n",
        "        # Selecciona la mejor acción entre las legales\n",
        "        legal_q_values = {action: q_values[0, action].item() for action in legal_action_indices}\n",
        "\n",
        "        # Buscar el índice con la mejor Q-value\n",
        "        best_action_index = max(legal_q_values, key=legal_q_values.get)\n",
        "\n",
        "        # Decodificar la acción de vuelta a (factory_num, tile_color, row_num) para devolverla\n",
        "        return decode_action(best_action_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNjMxE02AaQ3"
      },
      "outputs": [],
      "source": [
        "def train_step(states, actions, rewards, next_states, dones):\n",
        "  \"\"\"Realiza una iteración de entrenamiento en un batch de datos.\"\"\"\n",
        "\n",
        "   # Predicción del mejor Q-value para el siguiente estado usando la red principal\n",
        "  next_qs_argmax = main_nn(next_states).argmax(dim=-1, keepdim=True)\n",
        "\n",
        "  # Extraer el Q-value de la mejor acción usando la red objetivo\n",
        "  masked_next_qs = target_nn(next_states).gather(1, next_qs_argmax).squeeze()\n",
        "\n",
        "  # Valor objetivo\n",
        "  target = rewards + (1.0 - dones) * discount * masked_next_qs\n",
        "\n",
        "  # Calcula los Q-values actuales para las acciones tomadas\n",
        "  masked_qs = main_nn(states).gather(1, actions.unsqueeze(dim=-1)).squeeze()\n",
        "\n",
        "  # Calcular pérdida\n",
        "  loss = loss_fn(masked_qs, target.detach())\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  # Clipping de gradientes para prevenir explosión\n",
        "  torch.nn.utils.clip_grad_norm_(main_nn.parameters(), max_norm=1.0)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n-fOdZ9yzZ2"
      },
      "outputs": [],
      "source": [
        "def soft_update(target_model, main_model, tau=0.01):\n",
        "    for target_param, main_param in zip(target_model.parameters(), main_model.parameters()):\n",
        "        target_param.data.copy_(tau * main_param.data + (1.0 - tau) * target_param.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXKvZVrtBeD"
      },
      "source": [
        "# Retomar entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1y5ctxJt5p_"
      },
      "outputs": [],
      "source": [
        "# Cargar el estado guardado\n",
        "checkpoint = torch.load('/content/drive/My Drive/TFE/model.pth')\n",
        "\n",
        "# Cargar los pesos del modelo\n",
        "main_nn.load_state_dict(checkpoint['model_state_dict'])\n",
        "main_nn.train()\n",
        "\n",
        "# Configurar el optimizador y cargar su estado\n",
        "optimizer = torch.optim.Adam(main_nn.parameters(), weight_decay=1e-5)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Cargar epsilon\n",
        "epsilon = checkpoint['epsilon']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eikG_2Q81ML"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpFP-UMA8zSB"
      },
      "outputs": [],
      "source": [
        "# Hiperparametros\n",
        "num_episodes = 50000\n",
        "evaluation_episodes = 20\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.9998\n",
        "epsilon_final = 0.01\n",
        "batch_size = 64\n",
        "discount = 0.9\n",
        "buffer_size = 200000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTJBkNtt89Fp"
      },
      "outputs": [],
      "source": [
        "buffer = UniformBuffer(size=buffer_size, device=device)\n",
        "last_100_ep_rewards = []\n",
        "losses = []\n",
        "metrics = []\n",
        "\n",
        "for episode in range(num_episodes+1):\n",
        "    state = game.get_initial_state()\n",
        "    game.draw_tiles(state)\n",
        "    done = False\n",
        "    loss = None\n",
        "    ep_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Seleccionar acción\n",
        "        legal_actions = game.get_legal_moves(state)\n",
        "        state_vector = state_to_vector(state)\n",
        "        action = select_action(state_vector, epsilon, legal_actions)\n",
        "\n",
        "        # Obtener estado siguiente y recompensa con la accion seleccionada\n",
        "        next_state, reward = game.step(state, action)\n",
        "        reward /= 100\n",
        "        next_state_vector = state_to_vector(next_state)\n",
        "        ep_reward += reward\n",
        "\n",
        "        # Comprobar fin de ronda\n",
        "        if game.check_end_of_round(next_state):\n",
        "            next_state.move_tiles_to_wall()\n",
        "            done = game.check_end_of_game(next_state)\n",
        "            game.draw_tiles(next_state)\n",
        "\n",
        "        factory_num, tile_color, row_num = action\n",
        "        action_index = encode_action(factory_num, tile_color, row_num)\n",
        "\n",
        "        # Guardar la transición en el buffer\n",
        "        buffer.add(state_vector, action_index, reward, next_state_vector, done)\n",
        "\n",
        "        # Realizar el aprendizaje por batch\n",
        "        if len(buffer) > batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "            loss = train_step(states, actions, rewards, next_states, dones)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # Actualizar el estado actual\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if len(last_100_ep_rewards) == 100:\n",
        "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
        "    last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "    # Monitorear los valores Q para el estado actual\n",
        "    if episode % 50 == 0:\n",
        "\n",
        "        avg_reward = np.mean(last_100_ep_rewards)\n",
        "        avg_loss = np.mean(losses[-100:])\n",
        "        loss_value = loss.item() if loss is not None else None\n",
        "\n",
        "        # Cálculo del Q-value promedio\n",
        "        state_tensor = torch.tensor(state_vector, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_values = main_nn(state_tensor)\n",
        "        avg_q_value = q_values.mean().item()\n",
        "\n",
        "\n",
        "    # Reducir epsilon para explorar menos con el tiempo\n",
        "    epsilon = max(epsilon_final, epsilon_decay * epsilon)\n",
        "\n",
        "    # Aplicar el scheduler y controlar el learning rate hasta llegar al límite\n",
        "    scheduler.step()\n",
        "\n",
        "    # Verifica si el learning rate actual es menor que el mínimo permitido\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    if current_lr < min_lr:\n",
        "        # Si es menor, restablece el learning rate al valor mínimo\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = min_lr\n",
        "\n",
        "\n",
        "    soft_update(target_nn, main_nn, tau=0.01)\n",
        "\n",
        "\n",
        "    # Guardar los pesos del modelo principal\n",
        "    if episode % 200 == 0:\n",
        "        checkpoint = {\n",
        "          'episode': episode,  # Guardar el número de episodio actual\n",
        "          'model_state_dict': main_nn.state_dict(),  # Pesos del modelo\n",
        "          'optimizer_state_dict': optimizer.state_dict(),  # Estado del optimizador\n",
        "          'scheduler_state_dict': scheduler.state_dict(),\n",
        "          'memory' : buffer,\n",
        "          'epsilon': epsilon\n",
        "        }\n",
        "        torch.save(checkpoint, '/content/drive/My Drive/TFE/model.pth')\n",
        "\n",
        "\n",
        "    # Evaluación contra un jugador aleatorio\n",
        "    if episode % 100 == 0:\n",
        "        total_reward = 0\n",
        "        total_s1 = 0\n",
        "        total_s2 = 0\n",
        "        total_duration = 0\n",
        "\n",
        "        for eval_episode in range(evaluation_episodes):\n",
        "            random_state = game.get_initial_state()\n",
        "            game.draw_tiles(random_state)\n",
        "            done = False\n",
        "            random_reward = 0\n",
        "            DQN_reward = 0\n",
        "            actions = 0\n",
        "\n",
        "            while True:\n",
        "                if random_state.current_player == 0:\n",
        "                    legal_actions = game.get_legal_moves(random_state)\n",
        "                    action = select_action(state_to_vector(random_state), 0, legal_actions)\n",
        "                    next_state, reward = game.step(random_state, action)\n",
        "                    DQN_reward += reward\n",
        "                    actions += 1\n",
        "                else:\n",
        "                    action = game.random_player(random_state)\n",
        "                    next_state, reward = game.step(random_state, action)\n",
        "                    random_reward += reward\n",
        "\n",
        "                if game.check_end_of_round(next_state):\n",
        "                    next_state.move_tiles_to_wall()\n",
        "                    game.draw_tiles(next_state)\n",
        "\n",
        "                if game.check_end_of_game(next_state):\n",
        "                    done = True\n",
        "\n",
        "                random_state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            punt1 = random_state.players[0]['score']\n",
        "            punt2 = random_state.players[1]['score']\n",
        "\n",
        "            total_s1 += punt1\n",
        "            total_s2 += punt2\n",
        "            total_reward += DQN_reward\n",
        "            total_duration += actions\n",
        "\n",
        "        # Cálculos de promedios\n",
        "        average_bounty = total_reward / evaluation_episodes\n",
        "        average_s1 = total_s1 / evaluation_episodes\n",
        "        average_s2 = total_s2 / evaluation_episodes\n",
        "        average_duration = total_duration / evaluation_episodes\n",
        "\n",
        "        # Mostrar los resultados en la consola\n",
        "        print(f\"Episodio {episode}: Recompensa Promedio = {average_bounty:.4f}, Puntuación Promedio = {average_s1}, Punt random: {average_s2}, Acciones/partida = {average_duration}, lr: {current_lr}, Pérdida Promedio = {avg_loss}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO7rHgbxZnsy"
      },
      "source": [
        "# Evaluación modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr6w_nEVvw6z"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/content/drive/My Drive/TFE/model.pth')\n",
        "\n",
        "# Cargar los pesos del modelo\n",
        "main_nn.load_state_dict(checkpoint['model_state_dict'])\n",
        "main_nn.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tXImu9pAMI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "num_episodes = 1000  # Número de episodios (partidas) para evaluar\n",
        "total_score = 0\n",
        "total_s1 = 0\n",
        "total_s2 = 0\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = game.get_initial_state()\n",
        "    game.draw_tiles(state)\n",
        "    state_vector = state_to_vector(state)\n",
        "    done = False\n",
        "    episode_score = 0\n",
        "\n",
        "    while True:\n",
        "        #state.display_state()\n",
        "        if state.current_player == 0:\n",
        "            legal_actions = game.get_legal_moves(state)\n",
        "            action = select_action(state_vector, epsilon=0, legal_actions=legal_actions)  # epsilon=0 para evaluación\n",
        "            next_state, reward = game.step(state, action)  # Realizar la acción\n",
        "            episode_score += reward\n",
        "\n",
        "        else:\n",
        "            action = game.random_player(state)\n",
        "            next_state, reward = game.step(state, action)\n",
        "\n",
        "        # Actualizar el estado\n",
        "        state_vector = state_to_vector(next_state)\n",
        "\n",
        "        if game.check_end_of_round(state):\n",
        "            state.move_tiles_to_wall()\n",
        "            game.draw_tiles(state)\n",
        "\n",
        "        if game.check_end_of_game(next_state):\n",
        "                    done = True\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    punt1 = state.players[0]['score']\n",
        "    punt2 = state.players[1]['score']\n",
        "    total_score += episode_score\n",
        "    total_s1 += punt1\n",
        "    total_s2 += punt2\n",
        "\n",
        "\n",
        "    print(f\"Partida {episode + 1}: Recompensa del agente = {episode_score}\")\n",
        "    print(f\"Puntuacion 1: {punt1}, Puntuacion 2: {punt2}\")\n",
        "    input(\"Presiona Enter para continuar...\")\n",
        "\n",
        "# Calcular la puntuación promedio\n",
        "average_score = total_score / num_episodes\n",
        "average_s1 = total_s1 / num_episodes\n",
        "average_s2 = total_s2 / num_episodes\n",
        "average = (average_s1 + average_s2) / 2\n",
        "print(f\"Puntuación promedio del agente en {num_episodes} partidas: p1: {average_s1}, p2: {average_s2}, Recompensa: {average_score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
